{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZxk2sSDIxXL"
      },
      "source": [
        "# Demo: Loading Pre-Trained Speech Command Model from Google Drive\n",
        "This notebook demonstrates loading pre-trained model weights from Google Drive and running inference on an audio sample using the model.\n",
        "\n",
        "### Steps:\n",
        "1. Mount Google Drive.\n",
        "2. Load the pre-trained model weights.\n",
        "3. Prepare an audio sample for inference.\n",
        "4. Perform inference and display the predicted command.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2tb4zU3IxXM"
      },
      "outputs": [],
      "source": [
        "# Step 1: Mount Google Drive to access the model weights\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGuXiAaPIxXM"
      },
      "source": [
        "### Step 2: Load the Pre-Trained Model\n",
        "Make sure to provide the correct path to your saved model weights in Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9w6TpsDIxXM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "\n",
        "# Define the model architecture (make sure it matches the one used in training)\n",
        "class M5(nn.Module):\n",
        "    def __init__(self, n_input=1, n_output=35, stride=16, n_channel=32):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n",
        "        self.bn1 = nn.BatchNorm1d(n_channel)\n",
        "        self.pool1 = nn.MaxPool1d(4)\n",
        "        self.conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
        "        self.bn2 = nn.BatchNorm1d(n_channel)\n",
        "        self.pool2 = nn.MaxPool1d(4)\n",
        "        self.conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
        "        self.bn3 = nn.BatchNorm1d(2 * n_channel)\n",
        "        self.pool3 = nn.MaxPool1d(4)\n",
        "        self.conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
        "        self.bn4 = nn.BatchNorm1d(2 * n_channel)\n",
        "        self.pool4 = nn.MaxPool1d(4)\n",
        "        self.fc1 = nn.Linear(2 * n_channel, n_output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(self.bn1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(self.bn2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(self.bn3(x))\n",
        "        x = self.pool3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = F.relu(self.bn4(x))\n",
        "        x = self.pool4(x)\n",
        "        x = F.avg_pool1d(x, x.shape[-1])\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=2)\n",
        "\n",
        "# Instantiate the model\n",
        "model = SpeechCommandModel()\n",
        "\n",
        "# Load the pre-trained weights from Google Drive (update the path accordingly)\n",
        "model_path = '/content/drive/MyDrive/Lab_Eval1/weights.pth'\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TKiaDL_IxXN"
      },
      "source": [
        "### Step 3: Prepare an Audio Sample for Inference\n",
        "Load an audio sample and convert it to the format required by the model (e.g., spectrogram)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfDR-6AJIxXN"
      },
      "outputs": [],
      "source": [
        "# Load an audio file for inference (provide the path to the audio file)\n",
        "waveform, sample_rate = torchaudio.load('/content/drive/MyDrive/Lab_Eval1/dataset_user_001/down_1.wav')\n",
        "\n",
        "# Transform the waveform into a mel-spectrogram (required by the model)\n",
        "transform = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=16000,\n",
        "    n_mels=64\n",
        ")\n",
        "mel_spectrogram = transform(waveform)\n",
        "\n",
        "# Add batch dimension and adjust for model input\n",
        "input_tensor = mel_spectrogram.unsqueeze(0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9sSU4LEIxXN"
      },
      "source": [
        "### Step 4: Perform Inference and Display the Predicted Command\n",
        "Pass the processed audio through the model and interpret the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4McubmRdIxXN"
      },
      "outputs": [],
      "source": [
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    output = model(input_tensor)\n",
        "\n",
        "# Get the predicted label (assuming it's a classification task)\n",
        "predicted_label = torch.argmax(output, dim=1)\n",
        "\n",
        "# Define the list of commands (update according to your dataset)\n",
        "commands = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'zero', 'one', 'two', 'three', 'four',\n",
        "            'five', 'six', 'seven', 'eight', 'nine', 'bed', 'bird', 'cat', 'dog', 'happy', 'house', 'marvin',\n",
        "            'sheila', 'tree', 'wow', 'visual', 'backward', 'forward', 'follow', 'learn']\n",
        "\n",
        "# Display the predicted command\n",
        "print(f'Predicted command: {commands[predicted_label]}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}