{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanskarSureka/102103688-SESS_LE1/blob/main/102103688_SanskarSureka.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name: **Sanskar Sureka**  \n",
        "Email: `ssureka_be21@thapar.edu`  \n",
        "Roll No: **102103688**  \n",
        "Group: **4CO24**  \n",
        "Start Timestamp: 20240911-10:00  "
      ],
      "metadata": {
        "id": "JxSGFfaneMt9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question\n",
        "\n",
        "1. Read and summarise the paper in about 50 words.\n",
        "2. Download the dataset in the paper, statistically analyse and\n",
        "   describe it, so that it may be useful for posterity. (Include code\n",
        "   snippets in your .ipynb file to evidence your analysis.)\n",
        "3. Train a classifier so that you are able to distinguish the commands\n",
        "   in the dataset.\n",
        "4. Report the performance results using standard benchmarks.\n",
        "5. Record about 30 samples of each command in your voice and create a\n",
        "   new dataset (including a new user id for yourself).  You may use a\n",
        "   timer on your computer to synchronise.\n",
        "6. Fine tune your classifier to perform on your voice.\n",
        "7. Report the results."
      ],
      "metadata": {
        "id": "4rz1YOxxf4ON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution"
      ],
      "metadata": {
        "id": "qx2xTbgSFcWJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEwtgNH5dtho"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "! pip install pydub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchaudio\n",
        "import sys\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "ks4y9USPrsH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "61a6sHGkrudI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchaudio.datasets import SPEECHCOMMANDS\n",
        "import os\n",
        "\n",
        "\n",
        "class SubsetSC(SPEECHCOMMANDS):\n",
        "    def __init__(self, subset: str = None):\n",
        "        super().__init__(\"./\", download=True)\n",
        "\n",
        "        def load_list(filename):\n",
        "            filepath = os.path.join(self._path, filename)\n",
        "            with open(filepath) as fileobj:\n",
        "                return [os.path.normpath(os.path.join(self._path, line.strip())) for line in fileobj]\n",
        "\n",
        "        if subset == \"validation\":\n",
        "            self._walker = load_list(\"validation_list.txt\")\n",
        "        elif subset == \"testing\":\n",
        "            self._walker = load_list(\"testing_list.txt\")\n",
        "        elif subset == \"training\":\n",
        "            excludes = load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\")\n",
        "            excludes = set(excludes)\n",
        "            self._walker = [w for w in self._walker if w not in excludes]\n",
        "\n",
        "\n",
        "# Create training and testing split of the data. We do not use validation in this tutorial.\n",
        "train_set = SubsetSC(\"training\")\n",
        "test_set = SubsetSC(\"testing\")\n",
        "\n",
        "waveform, sample_rate, label, speaker_id, utterance_number = train_set[0]"
      ],
      "metadata": {
        "id": "4Zd6UroMrwda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of waveform: {}\".format(waveform.size()))\n",
        "print(\"Sample rate of waveform: {}\".format(sample_rate))\n",
        "\n",
        "plt.plot(waveform.t().numpy());"
      ],
      "metadata": {
        "id": "j8ubukcGrzoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = sorted(list(set(datapoint[2] for datapoint in train_set)))\n",
        "labels"
      ],
      "metadata": {
        "id": "P9tAamC2r15e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "waveform_first, *_ = train_set[0]\n",
        "ipd.Audio(waveform_first.numpy(), rate=sample_rate)\n",
        "\n",
        "waveform_second, *_ = train_set[1]\n",
        "ipd.Audio(waveform_second.numpy(), rate=sample_rate)"
      ],
      "metadata": {
        "id": "HkKL_PdQr4oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "waveform_last, *_ = train_set[-1]\n",
        "ipd.Audio(waveform_last.numpy(), rate=sample_rate)"
      ],
      "metadata": {
        "id": "0DHI9ik-r6_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_sample_rate = 8000\n",
        "transform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=new_sample_rate)\n",
        "transformed = transform(waveform)\n",
        "\n",
        "ipd.Audio(transformed.numpy(), rate=new_sample_rate)"
      ],
      "metadata": {
        "id": "iGOI04Jor9Iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_to_index(word):\n",
        "    # Return the position of the word in labels\n",
        "    return torch.tensor(labels.index(word))\n",
        "\n",
        "\n",
        "def index_to_label(index):\n",
        "    # Return the word corresponding to the index in labels\n",
        "    # This is the inverse of label_to_index\n",
        "    return labels[index]\n",
        "\n",
        "\n",
        "word_start = \"yes\"\n",
        "index = label_to_index(word_start)\n",
        "word_recovered = index_to_label(index)\n",
        "\n",
        "print(word_start, \"-->\", index, \"-->\", word_recovered)"
      ],
      "metadata": {
        "id": "XGSCmCuTsAb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_sequence(batch):\n",
        "    # Make all tensor in a batch the same length by padding with zeros\n",
        "    batch = [item.t() for item in batch]\n",
        "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.)\n",
        "    return batch.permute(0, 2, 1)\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "\n",
        "    # A data tuple has the form:\n",
        "    # waveform, sample_rate, label, speaker_id, utterance_number\n",
        "\n",
        "    tensors, targets = [], []\n",
        "\n",
        "    # Gather in lists, and encode labels as indices\n",
        "    for waveform, _, label, *_ in batch:\n",
        "        tensors += [waveform]\n",
        "        targets += [label_to_index(label)]\n",
        "\n",
        "    # Group the list of tensors into a batched tensor\n",
        "    tensors = pad_sequence(tensors)\n",
        "    targets = torch.stack(targets)\n",
        "\n",
        "    return tensors, targets\n",
        "\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "if device == \"cuda\":\n",
        "    num_workers = 1\n",
        "    pin_memory = True\n",
        "else:\n",
        "    num_workers = 0\n",
        "    pin_memory = False\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=pin_memory,\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=pin_memory,\n",
        ")"
      ],
      "metadata": {
        "id": "oVLnagDDsC19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class M5(nn.Module):\n",
        "    def __init__(self, n_input=1, n_output=35, stride=16, n_channel=32):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n",
        "        self.bn1 = nn.BatchNorm1d(n_channel)\n",
        "        self.pool1 = nn.MaxPool1d(4)\n",
        "        self.conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
        "        self.bn2 = nn.BatchNorm1d(n_channel)\n",
        "        self.pool2 = nn.MaxPool1d(4)\n",
        "        self.conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
        "        self.bn3 = nn.BatchNorm1d(2 * n_channel)\n",
        "        self.pool3 = nn.MaxPool1d(4)\n",
        "        self.conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
        "        self.bn4 = nn.BatchNorm1d(2 * n_channel)\n",
        "        self.pool4 = nn.MaxPool1d(4)\n",
        "        self.fc1 = nn.Linear(2 * n_channel, n_output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(self.bn1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(self.bn2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(self.bn3(x))\n",
        "        x = self.pool3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = F.relu(self.bn4(x))\n",
        "        x = self.pool4(x)\n",
        "        x = F.avg_pool1d(x, x.shape[-1])\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=2)\n",
        "\n",
        "\n",
        "model = M5(n_input=transformed.shape[0], n_output=len(labels))\n",
        "model.to(device)\n",
        "print(model)\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "n = count_parameters(model)\n",
        "print(\"Number of parameters: %s\" % n)"
      ],
      "metadata": {
        "id": "aPInVT8wsIhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)  # reduce the learning after 20 epochs by a factor of 10"
      ],
      "metadata": {
        "id": "94MurVOKsMZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, epoch, log_interval):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        # apply transform and model on whole batch directly on device\n",
        "        data = transform(data)\n",
        "        output = model(data)\n",
        "\n",
        "        # negative log-likelihood for a tensor of size (batch x 1 x n_output)\n",
        "        loss = F.nll_loss(output.squeeze(), target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print training stats\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
        "\n",
        "        # update progress bar\n",
        "        pbar.update(pbar_update)\n",
        "        # record loss\n",
        "        losses.append(loss.item())"
      ],
      "metadata": {
        "id": "jN9_WyAusO4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def number_of_correct(pred, target):\n",
        "    # count number of correct predictions\n",
        "    return pred.squeeze().eq(target).sum().item()\n",
        "\n",
        "\n",
        "def get_likely_index(tensor):\n",
        "    # find most likely label index for each element in the batch\n",
        "    return tensor.argmax(dim=-1)\n",
        "\n",
        "\n",
        "def test(model, epoch):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        # apply transform and model on whole batch directly on device\n",
        "        data = transform(data)\n",
        "        output = model(data)\n",
        "\n",
        "        pred = get_likely_index(output)\n",
        "        correct += number_of_correct(pred, target)\n",
        "\n",
        "        # update progress bar\n",
        "        pbar.update(pbar_update)\n",
        "\n",
        "    print(f\"\\nTest Epoch: {epoch}\\tAccuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n\")"
      ],
      "metadata": {
        "id": "P9HK9PmLsSF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_interval = 20\n",
        "n_epoch = 5\n",
        "\n",
        "pbar_update = 1 / (len(train_loader) + len(test_loader))\n",
        "losses = []\n",
        "\n",
        "# The transform needs to live on the same device as the model and the data.\n",
        "transform = transform.to(device)\n",
        "with tqdm(total=n_epoch) as pbar:\n",
        "    for epoch in range(1, n_epoch + 1):\n",
        "        train(model, epoch, log_interval)\n",
        "        test(model, epoch)\n",
        "        scheduler.step()\n",
        "\n",
        "# Let's plot the training loss versus the number of iteration.\n",
        "# plt.plot(losses);\n",
        "# plt.title(\"training loss\");"
      ],
      "metadata": {
        "id": "EK3PBBTfsVJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'm5_speech_command_classification_state_dict.pth')"
      ],
      "metadata": {
        "id": "Mf-9zPg4sXXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(tensor):\n",
        "    # Use the model to predict the label of the waveform\n",
        "    tensor = tensor.to(device)\n",
        "    tensor = transform(tensor)\n",
        "    tensor = model(tensor.unsqueeze(0))\n",
        "    tensor = get_likely_index(tensor)\n",
        "    tensor = index_to_label(tensor.squeeze())\n",
        "    return tensor\n",
        "\n",
        "\n",
        "waveform, sample_rate, utterance, *_ = train_set[-1]\n",
        "ipd.Audio(waveform.numpy(), rate=sample_rate)\n",
        "\n",
        "print(f\"Expected: {utterance}. Predicted: {predict(waveform)}.\")"
      ],
      "metadata": {
        "id": "0pJiGsYTsZ1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (waveform, sample_rate, utterance, *_) in enumerate(test_set):\n",
        "    output = predict(waveform)\n",
        "    if output != utterance:\n",
        "        ipd.Audio(waveform.numpy(), rate=sample_rate)\n",
        "        print(f\"Data point #{i}. Expected: {utterance}. Predicted: {output}.\")\n",
        "        break\n",
        "else:\n",
        "    print(\"All examples in this dataset were correctly classified!\")\n",
        "    print(\"In this case, let's just look at the last data point\")\n",
        "    ipd.Audio(waveform.numpy(), rate=sample_rate)\n",
        "    print(f\"Data point #{i}. Expected: {utterance}. Predicted: {output}.\")"
      ],
      "metadata": {
        "id": "O8jwzyMzscRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torchaudio\n",
        "\n",
        "# Define the directory where your dataset is stored\n",
        "dataset_dir = \"./my_speech_dataset\"\n",
        "\n",
        "# Create empty lists for the train and test sets\n",
        "train_set = []\n",
        "test_set = []\n",
        "\n",
        "# Define the commands and the number of instances per command\n",
        "commands = [f\"command_{i}\" for i in range(10)]\n",
        "num_instances = 30\n",
        "\n",
        "# Iterate over the commands and instances to load the data\n",
        "for command in commands:\n",
        "  for i in range(num_instances):\n",
        "    # Construct the file path\n",
        "    file_path = os.path.join(dataset_dir, command, f\"{command}_{i}.wav\") # Assumes your files are named command_i.wav\n",
        "\n",
        "    # Load the waveform and sample rate\n",
        "    waveform, sample_rate = torchaudio.load(file_path)\n",
        "\n",
        "    # Add the data to the appropriate set (e.g., 80% train, 20% test)\n",
        "    if i < 0.8 * num_instances:\n",
        "      train_set.append((waveform, sample_rate, command))\n",
        "    else:\n",
        "      test_set.append((waveform, sample_rate, command))"
      ],
      "metadata": {
        "id": "xlxR1w9jsebO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new labels for your dataset\n",
        "new_labels = [f\"command_{i}\" for i in range(10)]\n",
        "\n",
        "# Update the label_to_index and index_to_label functions\n",
        "def label_to_index(word):\n",
        "  return torch.tensor(new_labels.index(word))\n",
        "\n",
        "def index_to_label(index):\n",
        "  return new_labels[index]\n",
        "\n",
        "# Create data loaders for your dataset\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=pin_memory,\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=pin_memory,\n",
        ")\n",
        "\n",
        "# Modify the model's output layer to match the number of new labels\n",
        "model.fc1 = nn.Linear(2 * model.conv4.out_channels, len(new_labels))\n",
        "model.to(device)\n",
        "\n",
        "# Reset the optimizer and scheduler\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001) # Lower learning rate for finetuning\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
        "\n",
        "# Train and test the model\n",
        "n_epoch = 10 # Adjust as needed\n",
        "with tqdm(total=n_epoch) as pbar:\n",
        "  for epoch in range(1, n_epoch + 1):\n",
        "      train(model, epoch, log_interval)\n",
        "      test(model, epoch)\n",
        "      scheduler.step()"
      ],
      "metadata": {
        "id": "qw9JWz3QshV1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}