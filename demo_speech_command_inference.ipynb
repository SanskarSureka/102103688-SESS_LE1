{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Demo: Loading Pre-Trained Speech Command Model from Google Drive\n", "This notebook demonstrates loading pre-trained model weights from Google Drive and running inference on an audio sample using the model.\n", "\n", "### Steps:\n", "1. Mount Google Drive.\n", "2. Load the pre-trained model weights.\n", "3. Prepare an audio sample for inference.\n", "4. Perform inference and display the predicted command.\n"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["# Step 1: Mount Google Drive to access the model weights\n", "from google.colab import drive\n", "drive.mount('/content/drive')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Step 2: Load the Pre-Trained Model\n", "Make sure to provide the correct path to your saved model weights in Google Drive."]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["import torch\n", "import torch.nn as nn\n", "import torchaudio\n", "\n", "# Define the model architecture (make sure it matches the one used in training)\n", "class SpeechCommandModel(nn.Module):\n", "    def __init__(self):\n", "        super(SpeechCommandModel, self).__init__()\n", "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1)\n", "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1)\n", "        self.fc1 = nn.Linear(32*61*13, 128)\n", "        self.fc2 = nn.Linear(128, 35)  # Assuming 35 commands\n", "\n", "    def forward(self, x):\n", "        x = torch.relu(self.conv1(x))\n", "        x = torch.relu(self.conv2(x))\n", "        x = x.view(x.size(0), -1)\n", "        x = torch.relu(self.fc1(x))\n", "        x = self.fc2(x)\n", "        return x\n", "\n", "# Instantiate the model\n", "model = SpeechCommandModel()\n", "\n", "# Load the pre-trained weights from Google Drive (update the path accordingly)\n", "model_path = '/content/drive/MyDrive/path_to_your_model_weights.pth'\n", "model.load_state_dict(torch.load(model_path))\n", "model.eval()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Step 3: Prepare an Audio Sample for Inference\n", "Load an audio sample and convert it to the format required by the model (e.g., spectrogram)."]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["# Load an audio file for inference (provide the path to the audio file)\n", "waveform, sample_rate = torchaudio.load('/content/drive/MyDrive/sample_audio.wav')\n", "\n", "# Transform the waveform into a mel-spectrogram (required by the model)\n", "transform = torchaudio.transforms.MelSpectrogram(\n", "    sample_rate=16000,\n", "    n_mels=64\n", ")\n", "mel_spectrogram = transform(waveform)\n", "\n", "# Add batch dimension and adjust for model input\n", "input_tensor = mel_spectrogram.unsqueeze(0)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Step 4: Perform Inference and Display the Predicted Command\n", "Pass the processed audio through the model and interpret the result."]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["# Perform inference\n", "with torch.no_grad():\n", "    output = model(input_tensor)\n", "\n", "# Get the predicted label (assuming it's a classification task)\n", "predicted_label = torch.argmax(output, dim=1)\n", "\n", "# Define the list of commands (update according to your dataset)\n", "commands = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'zero', 'one', 'two', 'three', 'four',\n", "            'five', 'six', 'seven', 'eight', 'nine', 'bed', 'bird', 'cat', 'dog', 'happy', 'house', 'marvin',\n", "            'sheila', 'tree', 'wow', 'visual', 'backward', 'forward', 'follow', 'learn']\n", "\n", "# Display the predicted command\n", "print(f'Predicted command: {commands[predicted_label]}')"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.8"}}, "nbformat": 4, "nbformat_minor": 4}